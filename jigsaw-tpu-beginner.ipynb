{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU: grpc://10.0.0.2:8470\n"
     ]
    }
   ],
   "source": [
    "PATH_TPU_WORKER = ''\n",
    "\n",
    "def check_tpu():\n",
    "    \"\"\"\n",
    "    Detect TPU hardware and return the appopriate distribution strategy\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n",
    "        print('Running on TPU: {}'.format(tpu.master()))\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        tpu_strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        print(\"Num. replicas: {}\".format(tpu_strategy.num_replicas_in_sync))\n",
    "    \n",
    "    return tpu, tpu_strategy\n",
    "    \n",
    "tpu, tpu_strategy = check_tpu()\n",
    "PATH_TPU_WORKER = tpu.master()\n",
    "NUM_REPLICAS = tpu_strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CHALLENGE = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "\n",
    "PATH_TRAIN_FILENAME = PATH_CHALLENGE + \"jigsaw-toxic-comment-train.csv\"\n",
    "PATH_TEST_FILENAME = PATH_CHALLENGE + \"test.csv\"\n",
    "PATH_VALID_FILENAME = PATH_CHALLENGE + \"validation.csv\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "LOAD\n",
    "\"\"\"\n",
    "\n",
    "train_df = pd.read_csv(PATH_TRAIN_FILENAME)\n",
    "test_df = pd.read_csv(PATH_TEST_FILENAME)\n",
    "valid_df = pd.read_csv(PATH_VALID_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a437d36a16498ca4101268ea204766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256\n",
    "\n",
    "# Adapted https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n",
    "save_path = '/kaggle/working/distilbert_base_uncased/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "fast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(texts, tokenizer, chunk_size=256, maxlen=MAX_LEN):\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)\n",
    "\n",
    "x_train = encode(train_df.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = encode(valid_df.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_test = encode(test_df.content.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "y_train = train_df.toxic.values\n",
    "y_valid = valid_df.toxic.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Total batch size: 128\n"
     ]
    }
   ],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "BATCH_SIZE = 16 \n",
    "TOTAL_BATCH_SIZE = BATCH_SIZE * tpu_strategy.num_replicas_in_sync\n",
    "print(\"Batch size: {}\".format(BATCH_SIZE))\n",
    "print(\"Total batch size: {}\".format(TOTAL_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATA Loading\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(TOTAL_BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    #.repeat()\n",
    "    .batch(TOTAL_BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(TOTAL_BATCH_SIZE)\n",
    "    #.repeat()\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "words (InputLayer)           [(128, 256)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, 10)                 2570      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (128, 1)                  11        \n",
      "=================================================================\n",
      "Total params: 2,581\n",
      "Trainable params: 2,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def simple_model(max_len=MAX_LEN):\n",
    "    words = Input(shape=(max_len,), batch_size=TOTAL_BATCH_SIZE, dtype=tf.int32, name=\"words\")\n",
    "    x = Dense(10, activation='relu')(words)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model.summary()\n",
    "\n",
    "simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "words (InputLayer)           [(16, 256)]               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (16, 10)                  2570      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (16, 1)                   11        \n",
      "=================================================================\n",
      "Total params: 2,581\n",
      "Trainable params: 2,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tpu_strategy.scope():\n",
    "    simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 256, 512)     61208064    words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 256, 512)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 256, 512)     1574912     spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256, 512)     1574912     bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 512)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 512)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         1049600     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1024)         0           concatenate[0][0]                \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1024)         0           add[0][0]                        \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            1025        add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 66,458,113\n",
      "Trainable params: 66,458,113\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def lstm_model(vocab_size, max_len=MAX_LEN):\n",
    "    \n",
    "    words = Input(shape=(max_len,), dtype=tf.int32, name=\"words\")\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=512, input_length=max_len)(words)\n",
    "    x = tf.keras.layers.SpatialDropout1D(0.3)(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "\n",
    "    hidden = tf.keras.layers.concatenate([\n",
    "        tf.keras.layers.GlobalMaxPooling1D()(x),\n",
    "        tf.keras.layers.GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = tf.keras.layers.add([hidden, Dense(4 * 256, activation='relu')(hidden)])\n",
    "    hidden = tf.keras.layers.add([hidden, Dense(4 * 256, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    model = tf.keras.Model(inputs=words, outputs=result)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), # much more faster with \n",
    "                  optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\"\"\"\n",
    "BUILD\n",
    "\"\"\"\n",
    "\n",
    "with tpu_strategy.scope():\n",
    "    vocab_size = tokenizer.vocab_size # Distil\n",
    "    model = lstm_model(vocab_size)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 219 steps, validate for 63 steps\n",
      "Epoch 1/5\n",
      "219/219 [==============================] - 57s 259ms/step - loss: 0.2602 - accuracy: 0.9144 - val_loss: 0.6932 - val_accuracy: 0.5955\n",
      "Epoch 2/5\n",
      "219/219 [==============================] - 41s 187ms/step - loss: 0.1469 - accuracy: 0.9473 - val_loss: 0.5992 - val_accuracy: 0.7226\n",
      "Epoch 3/5\n",
      "219/219 [==============================] - 41s 188ms/step - loss: 0.1261 - accuracy: 0.9554 - val_loss: 0.4725 - val_accuracy: 0.8279\n",
      "Epoch 4/5\n",
      "219/219 [==============================] - 41s 187ms/step - loss: 0.1133 - accuracy: 0.9591 - val_loss: 0.4934 - val_accuracy: 0.8346\n",
      "Epoch 5/5\n",
      "219/219 [==============================] - 41s 187ms/step - loss: 0.1115 - accuracy: 0.9590 - val_loss: 0.5337 - val_accuracy: 0.8341\n",
      "AUC-ROC validation set: \n",
      "63/63 [==============================] - 5s 79ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6195944566535768"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "N_TRAIN_STEPS = 219\n",
    "N_VALID_STEPS = 63\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=N_TRAIN_STEPS,\n",
    "    validation_data=valid_dataset,\n",
    "    validation_steps=N_VALID_STEPS,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "\n",
    "def auc_roc(dataset, ground_truth):\n",
    "    from sklearn.metrics import roc_curve\n",
    "    y_pred_keras = model.predict(dataset, verbose=1).ravel()\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(ground_truth, y_pred_keras)\n",
    "    from sklearn.metrics import auc\n",
    "    return auc(fpr_keras, tpr_keras)\n",
    "\n",
    "print(\"AUC-ROC validation set: \")\n",
    "auc_roc(valid_dataset, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499/499 [==============================] - 21s 41ms/step\n",
      "Input size differs from output size. Input size: 63812, Output size: 63816\n",
      "Predicitions is divisible by \n",
      "Save submission to csv.\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_dataset, verbose=1).ravel()\n",
    "\n",
    "input_size = test_df.shape[0]\n",
    "output_size = predictions.shape[0]\n",
    "\n",
    "if input_size != output_size:\n",
    "    print(\"Input size differs from output size. Input size: {}, Output size: {}\".format(input_size,output_size))\n",
    "\n",
    "if output_size % NUM_REPLICAS == 0:\n",
    "    print(\"Predicitions is divisible by \".format(NUM_REPLICAS))\n",
    "    \n",
    "    \n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_df.id,\n",
    "    'toxic': predictions[:input_size]\n",
    "})\n",
    "\n",
    "print(\"Save submission to csv.\")\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09711744e70743bb869389b35b5435d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "1fd9cca27ee8451baad88bea82d14141": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "72cc2b76a83f407aba200ff70c13f49e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d5347d6f2da54e8c99b3445f31236dc0",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_780f9a696da544f29b671e9f90d10274",
       "value": " 996k/996k [00:00&lt;00:00, 1.46MB/s]"
      }
     },
     "780f9a696da544f29b671e9f90d10274": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b44d22ce83544819b4cabee777affbd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e4e3cd4d264e4bc9b8f643e14b26442a",
       "max": 995526.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_09711744e70743bb869389b35b5435d1",
       "value": 995526.0
      }
     },
     "c8a437d36a16498ca4101268ea204766": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b44d22ce83544819b4cabee777affbd9",
        "IPY_MODEL_72cc2b76a83f407aba200ff70c13f49e"
       ],
       "layout": "IPY_MODEL_1fd9cca27ee8451baad88bea82d14141"
      }
     },
     "d5347d6f2da54e8c99b3445f31236dc0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4e3cd4d264e4bc9b8f643e14b26442a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
